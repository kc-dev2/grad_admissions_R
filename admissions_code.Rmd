---
title: "Graduate Admissions Prediction"
author: "Kyle Choi"
date: "3/12/2021"
output: html_document
---
***All files related to this project can be found at https://github.com/kc-dev2/grad_admissions_R***

## Purpose

The purpose of this project is to practice exploring and creating models on sample data. More specifically, I have selected graduate admissions data listed on [Kaggle](https://www.kaggle.com/mohansacharya/graduate-admissions). 

While exploring the data, I will ask questions such as "what predictor seems to be most effective in predicting admission probability" and also plot graphs to help communicate/visualize these results. Afterwards, I will create a few linear models to predict admission probabilities and validate these models.

### Exploration

To begin, let us import the data along with necessary libraries:

```{r message=FALSE, results='hide'}
library(tidyverse)
library(ggplot2)
library(corrplot)
library(caret)
library(glmnet)
library(neuralnet)

adm <- read_csv("../data/admissions.csv")
```

For data "cleaning", I will check to ensure that there are no missing data. Additionally, any columns that need be renamed or have types changed will be fixed.
```{r}
sapply(adm, function(x) sum(is.na(x))) #counting total missing values in each column
str(adm); summary(adm) #checking basic info on data
adm <- rename(adm, GRE = "GRE Score", TOEFL = "TOEFL Score", U_Ranking = "University Rating", adm_chance = "Chance of Admit") #renaming columns

names(adm)
adm$Research <- as.factor(adm$Research) #changing 'Research' column from numerical to factor

str(adm$Research)
```

Let's check potential relationships between predictors and outcome variables. Instead of displaying the correlation plots using plot(), I use the corrplot() function provided by the "corrplot" package to create a visual correlation matrix. Since "Research" column is a factor column, we create a subset of data excluding it to allow for creation of the correlation matrix.
```{r}
plot(adm)
sub_adm <- adm[, !names(adm) %in% c("Research")] #remove "Research" column

corrplot(cor(sub_adm), type = "lower", addCoef.col = "black")
```

As displayed in the bottom row (where admission chance is the y-axis for all correlations) we can see that, apart from "Serial No" which is just an ID column, the predictors generally have a strong correlation with admission chance percentages. In particular, a person's Letter of Recommendation (LOR) score has the weakest correlation with admission chances (R=0.65) while his/her undergraduate GPA (CGPA) has the strongest correlation (R=0.88). 

### Modeling

Now, I will go into creating prediction models for our data. Since all variables apart from ID seem to have some correlation with admission chance, I'll first create a linear model that includes all the variables. Then, I'll create another linear model using both ridge and lasso regularization to allow for feature reduction/selection. Finally, I'll create a neural network as part of a more modern ML approach. K-Fold Cross validation (with k=10) will be used to quantify the performance of each model.

Before doing any model creation, let's exclude the "Serial No." column from our dataset:

```{r}
adm_mod <- adm[, !names(adm) %in% c("Serial No.")]
```

To make comparison of different models as equal as possible, I decided it would be best to have all the models trained and tested on the same datasets. As will be shown in the code below, the training and testing subsets are created during each iteration of the for-loop and all the models are run within the same iteration. 

Before creating and testing the models, I do some data standardization and formatting since regularization and neural networks run better with standardized data.

```{r}
norm_data <- sapply(adm_mod[,-8], function(x) if(is.numeric(x)){scale(x)} else x)
norm_data <- as_tibble(norm_data)
norm_data$adm_chance <- adm_mod$adm_chance
norm_data$Research <- as.factor(norm_data$Research)
```

Note that the only non-numeric column 'Research' was not standardized and left as a factor column. To be completely honest, I'm not exactly sure how much this effects the model results. My research suggests that there is not much literature on techniques to properly address binary/categorical predictors in more technical regression models such as regularization and neural networks.

The final step before running the cross validation for-loop is to find optimal $\lambda$ values for ridge and lasso regularization. To do so, I run the cv.glmnet() function on the the data with 10 folds and find the $\lambda$ that minimizes the average error. 

```{r}
X <- model.matrix(adm_chance ~ ., data=norm_data)[, -1]
y <- norm_data$adm_chance

lm_ridge <- cv.glmnet(x=X,y=y,alpha=0,standardize=FALSE)
opt_lam_r <- lm_ridge$lambda.min

lm_lasso <- cv.glmnet(x=X,y=y,alpha=1,standardize=FALSE)
opt_lam_l <- lm_lasso$lambda.min
```

Now that the setup is complete, I move onto running the 10-fold cross validation for all the models. Because the correlation plots suggested that the predictors share a linear relationship with the output "adm_chance", the four models that are run are all linear models. In order, the models are 

1. full model (no regularization)
2. ridge regression model 
3. lasso regression model
4. neural network model

```{r}
folds <- createFolds(adm$`Serial No.`, k=10)
res <- matrix(0, 10, 3)
for(i in 1:10) {
  s <- folds[[i]]
  train <- norm_data[-s, ]
  trainY <- train$adm_chance
  test <- norm_data[s, ]
  testY <- test$adm_chance
  
  #full linear
  model1 <- lm(adm_chance ~., data = train)
  pred1 <- predict(model1, test)
  err1 <- sum(abs(pred1 - testY))/length(testY) #M.A.E.
  
  #regularization setup
  trainX <- model.matrix(adm_chance ~ ., data=train)[, -1]
  testX <- model.matrix(adm_chance ~ ., data=test)[, -1]
  
  #ridge
  model2 <- glmnet(x=trainX,y=trainY,alpha=0,lambda=opt_lam_r)
  pred2 <- predict(model2, testX)
  err2 <- sum(abs(pred2 - testY))/length(testY)
  
  #lasso
  model3 <- glmnet(x=trainX,y=trainY,alpha=1,lambda=opt_lam_l)
  pred3 <- predict(model3, testX)
  err3 <- sum(abs(pred3 - testY))/length(testY)
  
  #neural network
  # maxs <- apply(train, 2, max) 
  # mins <- apply(train, 2, min)
  # scaled <- as.data.frame(scale(train, center = mins, 
  #                               scale = maxs - mins))
  
  # mydata <- sapply(train, function(x) if(is.numeric(x)){
  #   scale(x)
  # } else x)
  # 
  # nn <- neuralnet(adm_chance ~ GRE + TOEFL + U_Ranking + SOP + LOR + CGPA + Research, data = mydata, hidden = c(4,3), linear.output = TRUE, stepmax=1e7)
  # 
  # pr.nn <- compute(nn, testX)
  
  res[i,] = c(err1,err2,err3)
}

colMeans(res)
```

The results show that all 3 models performed almost exactly the same. This makes sense when looking at the $/lambda$ values that were calculated earlier, as both values are very close to 0, which means that the coefficients are barely being penalized and thus the regularized models are very similar to the full, non-regularized model. 

<!-- #### 1. Full Model -->
<!-- We create linear models using the lm() function. -->

<!-- ```{r} -->
<!-- set.seed(22) # set seed for reproducible results -->
<!-- full_lm <- train(adm_chance ~ ., data=adm_mod, method="lm", trControl=training) -->
<!-- print(full_lm) -->
<!-- ``` -->

<!-- This model gives a decent result, with the predictors being able to explain 82% of the variance in the data ($R^{2}$) and having an average error of ~4% ($MAE$). -->

<!-- #### 2. Regularized Model -->
<!-- For regularized models, there are two steps that I take. First, I find the $\lambda$ that optimizes the model, and then test the full model, which now includes the $\lambda$ term, with cross validation. Note that to determine the optimal $\lambda$, another cross validation is required to test out performance of the models against the set of $\lambda$ values. -->

## Steps to take next

1. Create full model (lm)
~~https://www.geeksforgeeks.org/cross-validation-in-r-programming/~~
https://www.journaldev.com/46754/k-fold-cross-validation-in-r
2. Feature selection model using regularization (lm)
https://towardsdatascience.com/feature-selection-using-regularisation-a3678b71e499
https://uc-r.github.io/regularized_regression
3. Create full model (neural network)
https://www.geeksforgeeks.org/how-neural-networks-are-used-for-regression-in-r-programming/
https://datascienceplus.com/fitting-neural-network-in-r/
4?. Feature selection model using regularization (neural network)
5. Cross validation using k fold to test overall performance of models create in steps 1-4.
https://www.geeksforgeeks.org/cross-validation-in-r-programming/
https://www.statology.org/k-fold-cross-validation-in-r/
6. Select best performing model as prediction model.

Feature importance: https://machinelearningmastery.com/feature-selection-with-the-caret-r-package/

We want to create models that are simpler to avoid overfitting and to also be more cost/time efficient when creating predictions. We can also remove predictors that have collinearity and thus are redundant.